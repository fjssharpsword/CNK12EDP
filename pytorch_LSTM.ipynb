{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']), (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]\n"
     ]
    }
   ],
   "source": [
    "training_data = [(\"The dog ate the apple\".split(),\n",
    "                  [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "                 (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\",\n",
    "                                                       \"NN\"])]\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'Everybody': 5, 'apple': 4, 'the': 3, 'book': 8, 'that': 7, 'ate': 2, 'read': 6}\n",
      "{'DET': 0, 'NN': 1, 'V': 2}\n",
      "{'o': 14, 'v': 21, 'i': 8, 'g': 6, 'h': 7, 'l': 11, 'd': 3, 'f': 5, 'a': 0, 'w': 22, 'e': 4, 'x': 23, 'u': 20, 'y': 24, 't': 19, 'm': 12, 's': 18, 'z': 25, 'r': 17, 'b': 1, 'k': 10, 'j': 9, 'n': 13, 'q': 16, 'c': 2, 'p': 15}\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "for context, tag in training_data:\n",
    "    for word in context:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "    for label in tag:\n",
    "        if label not in tag_to_idx:\n",
    "            tag_to_idx[label] = len(tag_to_idx)\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "character_to_idx = {}\n",
    "for i in range(len(alphabet)):\n",
    "    character_to_idx[alphabet[i]] = i\n",
    "print(word_to_idx)\n",
    "print(tag_to_idx)\n",
    "print(character_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, n_char, char_dim, char_hidden):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.char_embedding = nn.Embedding(n_char, char_dim)\n",
    "        self.char_lstm = nn.LSTM(char_dim, char_hidden, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.char_embedding(x)\n",
    "        _, h = self.char_lstm(x)\n",
    "        return h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, n_word, n_char, char_dim, n_dim, char_hidden, n_hidden,\n",
    "                 n_tag):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(n_word, n_dim)\n",
    "        self.char_lstm = CharLSTM(n_char, char_dim, char_hidden)\n",
    "        self.lstm = nn.LSTM(n_dim + char_hidden, n_hidden, batch_first=True)\n",
    "        self.linear1 = nn.Linear(n_hidden, n_tag)\n",
    "\n",
    "    def forward(self, x, word):\n",
    "        char = torch.FloatTensor()\n",
    "        for each in word:\n",
    "            char_list = []\n",
    "            for letter in each:\n",
    "                char_list.append(character_to_idx[letter.lower()])\n",
    "            char_list = torch.LongTensor(char_list)\n",
    "            char_list = char_list.unsqueeze(0)\n",
    "            if torch.cuda.is_available():\n",
    "                tempchar = self.char_lstm(Variable(char_list).cuda())\n",
    "            else:\n",
    "                tempchar = self.char_lstm(Variable(char_list))\n",
    "            tempchar = tempchar.squeeze(0)\n",
    "            char = torch.cat((char, tempchar.cpu().data), 0)\n",
    "        if torch.cuda.is_available():\n",
    "            char = char.cuda()\n",
    "        char = Variable(char)\n",
    "        x = self.word_embedding(x)\n",
    "        x = torch.cat((x, char), 1)\n",
    "        x = x.unsqueeze(0)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.linear1(x)\n",
    "        y = F.log_softmax(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    len(word_to_idx), len(character_to_idx), 10, 100, 50, 128, len(tag_to_idx))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequence(x, dic):\n",
    "    idx = [dic[i] for i in x]\n",
    "    idx = Variable(torch.LongTensor(idx))\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "epoch 1\n",
      "Loss: 1.0878206491470337\n",
      "**********\n",
      "epoch 2\n",
      "Loss: 1.0831924080848694\n",
      "**********\n",
      "epoch 3\n",
      "Loss: 1.0785991549491882\n",
      "**********\n",
      "epoch 4\n",
      "Loss: 1.0740392804145813\n",
      "**********\n",
      "epoch 5\n",
      "Loss: 1.0695111751556396\n",
      "**********\n",
      "epoch 6\n",
      "Loss: 1.0650132894515991\n",
      "**********\n",
      "epoch 7\n",
      "Loss: 1.0605440735816956\n",
      "**********\n",
      "epoch 8\n",
      "Loss: 1.0561020374298096\n",
      "**********\n",
      "epoch 9\n",
      "Loss: 1.0516858100891113\n",
      "**********\n",
      "epoch 10\n",
      "Loss: 1.0472939610481262\n",
      "**********\n",
      "epoch 11\n",
      "Loss: 1.0429250597953796\n",
      "**********\n",
      "epoch 12\n",
      "Loss: 1.038577914237976\n",
      "**********\n",
      "epoch 13\n",
      "Loss: 1.03425133228302\n",
      "**********\n",
      "epoch 14\n",
      "Loss: 1.0299437642097473\n",
      "**********\n",
      "epoch 15\n",
      "Loss: 1.0256543159484863\n",
      "**********\n",
      "epoch 16\n",
      "Loss: 1.021381676197052\n",
      "**********\n",
      "epoch 17\n",
      "Loss: 1.0171247720718384\n",
      "**********\n",
      "epoch 18\n",
      "Loss: 1.01288241147995\n",
      "**********\n",
      "epoch 19\n",
      "Loss: 1.008653700351715\n",
      "**********\n",
      "epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0044374465942383\n",
      "**********\n",
      "epoch 21\n",
      "Loss: 1.0002326965332031\n",
      "**********\n",
      "epoch 22\n",
      "Loss: 0.9960386157035828\n",
      "**********\n",
      "epoch 23\n",
      "Loss: 0.9918541312217712\n",
      "**********\n",
      "epoch 24\n",
      "Loss: 0.9876784980297089\n",
      "**********\n",
      "epoch 25\n",
      "Loss: 0.9835106730461121\n",
      "**********\n",
      "epoch 26\n",
      "Loss: 0.9793499410152435\n",
      "**********\n",
      "epoch 27\n",
      "Loss: 0.9751953184604645\n",
      "**********\n",
      "epoch 28\n",
      "Loss: 0.9710462391376495\n",
      "**********\n",
      "epoch 29\n",
      "Loss: 0.9669018983840942\n",
      "**********\n",
      "epoch 30\n",
      "Loss: 0.9627615809440613\n",
      "**********\n",
      "epoch 31\n",
      "Loss: 0.9586243331432343\n",
      "**********\n",
      "epoch 32\n",
      "Loss: 0.9544897973537445\n",
      "**********\n",
      "epoch 33\n",
      "Loss: 0.9503572881221771\n",
      "**********\n",
      "epoch 34\n",
      "Loss: 0.9462260007858276\n",
      "**********\n",
      "epoch 35\n",
      "Loss: 0.942095547914505\n",
      "**********\n",
      "epoch 36\n",
      "Loss: 0.9379652142524719\n",
      "**********\n",
      "epoch 37\n",
      "Loss: 0.9338345527648926\n",
      "**********\n",
      "epoch 38\n",
      "Loss: 0.9297029972076416\n",
      "**********\n",
      "epoch 39\n",
      "Loss: 0.9255701303482056\n",
      "**********\n",
      "epoch 40\n",
      "Loss: 0.9214353263378143\n",
      "**********\n",
      "epoch 41\n",
      "Loss: 0.9172983467578888\n",
      "**********\n",
      "epoch 42\n",
      "Loss: 0.9131585359573364\n",
      "**********\n",
      "epoch 43\n",
      "Loss: 0.9090156257152557\n",
      "**********\n",
      "epoch 44\n",
      "Loss: 0.9048692286014557\n",
      "**********\n",
      "epoch 45\n",
      "Loss: 0.9007189273834229\n",
      "**********\n",
      "epoch 46\n",
      "Loss: 0.8965645730495453\n",
      "**********\n",
      "epoch 47\n",
      "Loss: 0.8924056887626648\n",
      "**********\n",
      "epoch 48\n",
      "Loss: 0.8882420361042023\n",
      "**********\n",
      "epoch 49\n",
      "Loss: 0.8840732872486115\n",
      "**********\n",
      "epoch 50\n",
      "Loss: 0.8798993527889252\n",
      "**********\n",
      "epoch 51\n",
      "Loss: 0.8757197260856628\n",
      "**********\n",
      "epoch 52\n",
      "Loss: 0.871534526348114\n",
      "**********\n",
      "epoch 53\n",
      "Loss: 0.8673434257507324\n",
      "**********\n",
      "epoch 54\n",
      "Loss: 0.8631461560726166\n",
      "**********\n",
      "epoch 55\n",
      "Loss: 0.8589427173137665\n",
      "**********\n",
      "epoch 56\n",
      "Loss: 0.8547329902648926\n",
      "**********\n",
      "epoch 57\n",
      "Loss: 0.8505168259143829\n",
      "**********\n",
      "epoch 58\n",
      "Loss: 0.846294105052948\n",
      "**********\n",
      "epoch 59\n",
      "Loss: 0.8420647382736206\n",
      "**********\n",
      "epoch 60\n",
      "Loss: 0.8378288149833679\n",
      "**********\n",
      "epoch 61\n",
      "Loss: 0.8335862457752228\n",
      "**********\n",
      "epoch 62\n",
      "Loss: 0.8293369710445404\n",
      "**********\n",
      "epoch 63\n",
      "Loss: 0.8250810205936432\n",
      "**********\n",
      "epoch 64\n",
      "Loss: 0.8208183348178864\n",
      "**********\n",
      "epoch 65\n",
      "Loss: 0.8165490925312042\n",
      "**********\n",
      "epoch 66\n",
      "Loss: 0.8122733235359192\n",
      "**********\n",
      "epoch 67\n",
      "Loss: 0.8079909980297089\n",
      "**********\n",
      "epoch 68\n",
      "Loss: 0.8037023544311523\n",
      "**********\n",
      "epoch 69\n",
      "Loss: 0.799407422542572\n",
      "**********\n",
      "epoch 70\n",
      "Loss: 0.7951063215732574\n",
      "**********\n",
      "epoch 71\n",
      "Loss: 0.7907992303371429\n",
      "**********\n",
      "epoch 72\n",
      "Loss: 0.7864862382411957\n",
      "**********\n",
      "epoch 73\n",
      "Loss: 0.7821674644947052\n",
      "**********\n",
      "epoch 74\n",
      "Loss: 0.7778432965278625\n",
      "**********\n",
      "epoch 75\n",
      "Loss: 0.7735137045383453\n",
      "**********\n",
      "epoch 76\n",
      "Loss: 0.7691790461540222\n",
      "**********\n",
      "epoch 77\n",
      "Loss: 0.7648394405841827\n",
      "**********\n",
      "epoch 78\n",
      "Loss: 0.7604951858520508\n",
      "**********\n",
      "epoch 79\n",
      "Loss: 0.7561464607715607\n",
      "**********\n",
      "epoch 80\n",
      "Loss: 0.751793622970581\n",
      "**********\n",
      "epoch 81\n",
      "Loss: 0.7474370002746582\n",
      "**********\n",
      "epoch 82\n",
      "Loss: 0.7430766820907593\n",
      "**********\n",
      "epoch 83\n",
      "Loss: 0.7387130558490753\n",
      "**********\n",
      "epoch 84\n",
      "Loss: 0.7343464493751526\n",
      "**********\n",
      "epoch 85\n",
      "Loss: 0.729977160692215\n",
      "**********\n",
      "epoch 86\n",
      "Loss: 0.7256055176258087\n",
      "**********\n",
      "epoch 87\n",
      "Loss: 0.7212318480014801\n",
      "**********\n",
      "epoch 88\n",
      "Loss: 0.7168563902378082\n",
      "**********\n",
      "epoch 89\n",
      "Loss: 0.7124797105789185\n",
      "**********\n",
      "epoch 90\n",
      "Loss: 0.7081020474433899\n",
      "**********\n",
      "epoch 91\n",
      "Loss: 0.7037238478660583\n",
      "**********\n",
      "epoch 92\n",
      "Loss: 0.6993453800678253\n",
      "**********\n",
      "epoch 93\n",
      "Loss: 0.6949670612812042\n",
      "**********\n",
      "epoch 94\n",
      "Loss: 0.6905892789363861\n",
      "**********\n",
      "epoch 95\n",
      "Loss: 0.6862125098705292\n",
      "**********\n",
      "epoch 96\n",
      "Loss: 0.6818370819091797\n",
      "**********\n",
      "epoch 97\n",
      "Loss: 0.6774633824825287\n",
      "**********\n",
      "epoch 98\n",
      "Loss: 0.6730918288230896\n",
      "**********\n",
      "epoch 99\n",
      "Loss: 0.668722927570343\n",
      "**********\n",
      "epoch 100\n",
      "Loss: 0.6643569767475128\n",
      "**********\n",
      "epoch 101\n",
      "Loss: 0.6599946022033691\n",
      "**********\n",
      "epoch 102\n",
      "Loss: 0.6556359827518463\n",
      "**********\n",
      "epoch 103\n",
      "Loss: 0.6512816250324249\n",
      "**********\n",
      "epoch 104\n",
      "Loss: 0.6469320952892303\n",
      "**********\n",
      "epoch 105\n",
      "Loss: 0.6425876915454865\n",
      "**********\n",
      "epoch 106\n",
      "Loss: 0.6382488906383514\n",
      "**********\n",
      "epoch 107\n",
      "Loss: 0.6339161694049835\n",
      "**********\n",
      "epoch 108\n",
      "Loss: 0.6295899152755737\n",
      "**********\n",
      "epoch 109\n",
      "Loss: 0.625270664691925\n",
      "**********\n",
      "epoch 110\n",
      "Loss: 0.6209586262702942\n",
      "**********\n",
      "epoch 111\n",
      "Loss: 0.6166545152664185\n",
      "**********\n",
      "epoch 112\n",
      "Loss: 0.6123586297035217\n",
      "**********\n",
      "epoch 113\n",
      "Loss: 0.6080714464187622\n",
      "**********\n",
      "epoch 114\n",
      "Loss: 0.6037933230400085\n",
      "**********\n",
      "epoch 115\n",
      "Loss: 0.5995248258113861\n",
      "**********\n",
      "epoch 116\n",
      "Loss: 0.5952663123607635\n",
      "**********\n",
      "epoch 117\n",
      "Loss: 0.5910181999206543\n",
      "**********\n",
      "epoch 118\n",
      "Loss: 0.5867809951305389\n",
      "**********\n",
      "epoch 119\n",
      "Loss: 0.5825549960136414\n",
      "**********\n",
      "epoch 120\n",
      "Loss: 0.5783407688140869\n",
      "**********\n",
      "epoch 121\n",
      "Loss: 0.5741386413574219\n",
      "**********\n",
      "epoch 122\n",
      "Loss: 0.5699490308761597\n",
      "**********\n",
      "epoch 123\n",
      "Loss: 0.5657723546028137\n",
      "**********\n",
      "epoch 124\n",
      "Loss: 0.5616091191768646\n",
      "**********\n",
      "epoch 125\n",
      "Loss: 0.5574595332145691\n",
      "**********\n",
      "epoch 126\n",
      "Loss: 0.5533241033554077\n",
      "**********\n",
      "epoch 127\n",
      "Loss: 0.5492032617330551\n",
      "**********\n",
      "epoch 128\n",
      "Loss: 0.545097291469574\n",
      "**********\n",
      "epoch 129\n",
      "Loss: 0.5410066246986389\n",
      "**********\n",
      "epoch 130\n",
      "Loss: 0.5369316190481186\n",
      "**********\n",
      "epoch 131\n",
      "Loss: 0.5328726917505264\n",
      "**********\n",
      "epoch 132\n",
      "Loss: 0.5288300514221191\n",
      "**********\n",
      "epoch 133\n",
      "Loss: 0.5248042643070221\n",
      "**********\n",
      "epoch 134\n",
      "Loss: 0.5207955092191696\n",
      "**********\n",
      "epoch 135\n",
      "Loss: 0.5168041884899139\n",
      "**********\n",
      "epoch 136\n",
      "Loss: 0.5128306448459625\n",
      "**********\n",
      "epoch 137\n",
      "Loss: 0.5088751763105392\n",
      "**********\n",
      "epoch 138\n",
      "Loss: 0.504938080906868\n",
      "**********\n",
      "epoch 139\n",
      "Loss: 0.5010197162628174\n",
      "**********\n",
      "epoch 140\n",
      "Loss: 0.49712032079696655\n",
      "**********\n",
      "epoch 141\n",
      "Loss: 0.49324028193950653\n",
      "**********\n",
      "epoch 142\n",
      "Loss: 0.48937979340553284\n",
      "**********\n",
      "epoch 143\n",
      "Loss: 0.48553915321826935\n",
      "**********\n",
      "epoch 144\n",
      "Loss: 0.48171867430210114\n",
      "**********\n",
      "epoch 145\n",
      "Loss: 0.47791852056980133\n",
      "**********\n",
      "epoch 146\n",
      "Loss: 0.4741390347480774\n",
      "**********\n",
      "epoch 147\n",
      "Loss: 0.4703804701566696\n",
      "**********\n",
      "epoch 148\n",
      "Loss: 0.46664294600486755\n",
      "**********\n",
      "epoch 149\n",
      "Loss: 0.4629267603158951\n",
      "**********\n",
      "epoch 150\n",
      "Loss: 0.4592322111129761\n",
      "**********\n",
      "epoch 151\n",
      "Loss: 0.4555594176054001\n",
      "**********\n",
      "epoch 152\n",
      "Loss: 0.45190855860710144\n",
      "**********\n",
      "epoch 153\n",
      "Loss: 0.44827981293201447\n",
      "**********\n",
      "epoch 154\n",
      "Loss: 0.44467346370220184\n",
      "**********\n",
      "epoch 155\n",
      "Loss: 0.4410896301269531\n",
      "**********\n",
      "epoch 156\n",
      "Loss: 0.43752846121788025\n",
      "**********\n",
      "epoch 157\n",
      "Loss: 0.43399015069007874\n",
      "**********\n",
      "epoch 158\n",
      "Loss: 0.4304748475551605\n",
      "**********\n",
      "epoch 159\n",
      "Loss: 0.4269825667142868\n",
      "**********\n",
      "epoch 160\n",
      "Loss: 0.4235136806964874\n",
      "**********\n",
      "epoch 161\n",
      "Loss: 0.4200681447982788\n",
      "**********\n",
      "epoch 162\n",
      "Loss: 0.4166460633277893\n",
      "**********\n",
      "epoch 163\n",
      "Loss: 0.41324764490127563\n",
      "**********\n",
      "epoch 164\n",
      "Loss: 0.409872904419899\n",
      "**********\n",
      "epoch 165\n",
      "Loss: 0.4065220057964325\n",
      "**********\n",
      "epoch 166\n",
      "Loss: 0.40319497883319855\n",
      "**********\n",
      "epoch 167\n",
      "Loss: 0.3998919129371643\n",
      "**********\n",
      "epoch 168\n",
      "Loss: 0.39661289751529694\n",
      "**********\n",
      "epoch 169\n",
      "Loss: 0.39335797727108\n",
      "**********\n",
      "epoch 170\n",
      "Loss: 0.39012716710567474\n",
      "**********\n",
      "epoch 171\n",
      "Loss: 0.38692058622837067\n",
      "**********\n",
      "epoch 172\n",
      "Loss: 0.3837382048368454\n",
      "**********\n",
      "epoch 173\n",
      "Loss: 0.3805801123380661\n",
      "**********\n",
      "epoch 174\n",
      "Loss: 0.37744633853435516\n",
      "**********\n",
      "epoch 175\n",
      "Loss: 0.3743368983268738\n",
      "**********\n",
      "epoch 176\n",
      "Loss: 0.3712517023086548\n",
      "**********\n",
      "epoch 177\n",
      "Loss: 0.36819083988666534\n",
      "**********\n",
      "epoch 178\n",
      "Loss: 0.3651543855667114\n",
      "**********\n",
      "epoch 179\n",
      "Loss: 0.3621421903371811\n",
      "**********\n",
      "epoch 180\n",
      "Loss: 0.3591542989015579\n",
      "**********\n",
      "epoch 181\n",
      "Loss: 0.35619066655635834\n",
      "**********\n",
      "epoch 182\n",
      "Loss: 0.3532513529062271\n",
      "**********\n",
      "epoch 183\n",
      "Loss: 0.3503362536430359\n",
      "**********\n",
      "epoch 184\n",
      "Loss: 0.3474453538656235\n",
      "**********\n",
      "epoch 185\n",
      "Loss: 0.3445785492658615\n",
      "**********\n",
      "epoch 186\n",
      "Loss: 0.3417358547449112\n",
      "**********\n",
      "epoch 187\n",
      "Loss: 0.33891725540161133\n",
      "**********\n",
      "epoch 188\n",
      "Loss: 0.33612260222435\n",
      "**********\n",
      "epoch 189\n",
      "Loss: 0.33335188031196594\n",
      "**********\n",
      "epoch 190\n",
      "Loss: 0.33060501515865326\n",
      "**********\n",
      "epoch 191\n",
      "Loss: 0.32788190245628357\n",
      "**********\n",
      "epoch 192\n",
      "Loss: 0.3251825124025345\n",
      "**********\n",
      "epoch 193\n",
      "Loss: 0.32250677049160004\n",
      "**********\n",
      "epoch 194\n",
      "Loss: 0.3198545277118683\n",
      "**********\n",
      "epoch 195\n",
      "Loss: 0.31722576916217804\n",
      "**********\n",
      "epoch 196\n",
      "Loss: 0.31462031602859497\n",
      "**********\n",
      "epoch 197\n",
      "Loss: 0.3120381385087967\n",
      "**********\n",
      "epoch 198\n",
      "Loss: 0.30947910249233246\n",
      "**********\n",
      "epoch 199\n",
      "Loss: 0.3069431334733963\n",
      "**********\n",
      "epoch 200\n",
      "Loss: 0.3044300526380539\n",
      "**********\n",
      "epoch 201\n",
      "Loss: 0.30193984508514404\n",
      "**********\n",
      "epoch 202\n",
      "Loss: 0.2994723469018936\n",
      "**********\n",
      "epoch 203\n",
      "Loss: 0.2970273941755295\n",
      "**********\n",
      "epoch 204\n",
      "Loss: 0.29460495710372925\n",
      "**********\n",
      "epoch 205\n",
      "Loss: 0.2922048419713974\n",
      "**********\n",
      "epoch 206\n",
      "Loss: 0.2898269444704056\n",
      "**********\n",
      "epoch 207\n",
      "Loss: 0.28747114539146423\n",
      "**********\n",
      "epoch 208\n",
      "Loss: 0.2851372957229614\n",
      "**********\n",
      "epoch 209\n",
      "Loss: 0.282825268805027\n",
      "**********\n",
      "epoch 210\n",
      "Loss: 0.2805349677801132\n",
      "**********\n",
      "epoch 211\n",
      "Loss: 0.27826617658138275\n",
      "**********\n",
      "epoch 212\n",
      "Loss: 0.27601881325244904\n",
      "**********\n",
      "epoch 213\n",
      "Loss: 0.27379272878170013\n",
      "**********\n",
      "epoch 214\n",
      "Loss: 0.2715877592563629\n",
      "**********\n",
      "epoch 215\n",
      "Loss: 0.26940377056598663\n",
      "**********\n",
      "epoch 216\n",
      "Loss: 0.26724059134721756\n",
      "**********\n",
      "epoch 217\n",
      "Loss: 0.26509813219308853\n",
      "**********\n",
      "epoch 218\n",
      "Loss: 0.2629762068390846\n",
      "**********\n",
      "epoch 219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2608746513724327\n",
      "**********\n",
      "epoch 220\n",
      "Loss: 0.2587933838367462\n",
      "**********\n",
      "epoch 221\n",
      "Loss: 0.25673212110996246\n",
      "**********\n",
      "epoch 222\n",
      "Loss: 0.25469083338975906\n",
      "**********\n",
      "epoch 223\n",
      "Loss: 0.2526693046092987\n",
      "**********\n",
      "epoch 224\n",
      "Loss: 0.25066740065813065\n",
      "**********\n",
      "epoch 225\n",
      "Loss: 0.24868492782115936\n",
      "**********\n",
      "epoch 226\n",
      "Loss: 0.2467217817902565\n",
      "**********\n",
      "epoch 227\n",
      "Loss: 0.24477776885032654\n",
      "**********\n",
      "epoch 228\n",
      "Loss: 0.24285274744033813\n",
      "**********\n",
      "epoch 229\n",
      "Loss: 0.24094657599925995\n",
      "**********\n",
      "epoch 230\n",
      "Loss: 0.23905903100967407\n",
      "**********\n",
      "epoch 231\n",
      "Loss: 0.23719006776809692\n",
      "**********\n",
      "epoch 232\n",
      "Loss: 0.2353394255042076\n",
      "**********\n",
      "epoch 233\n",
      "Loss: 0.2335069626569748\n",
      "**********\n",
      "epoch 234\n",
      "Loss: 0.23169253766536713\n",
      "**********\n",
      "epoch 235\n",
      "Loss: 0.22989597916603088\n",
      "**********\n",
      "epoch 236\n",
      "Loss: 0.22811711579561234\n",
      "**********\n",
      "epoch 237\n",
      "Loss: 0.2263558730483055\n",
      "**********\n",
      "epoch 238\n",
      "Loss: 0.22461198270320892\n",
      "**********\n",
      "epoch 239\n",
      "Loss: 0.2228853553533554\n",
      "**********\n",
      "epoch 240\n",
      "Loss: 0.22117580473423004\n",
      "**********\n",
      "epoch 241\n",
      "Loss: 0.2194831520318985\n",
      "**********\n",
      "epoch 242\n",
      "Loss: 0.21780727803707123\n",
      "**********\n",
      "epoch 243\n",
      "Loss: 0.2161480188369751\n",
      "**********\n",
      "epoch 244\n",
      "Loss: 0.21450519561767578\n",
      "**********\n",
      "epoch 245\n",
      "Loss: 0.21287868916988373\n",
      "**********\n",
      "epoch 246\n",
      "Loss: 0.21126826852560043\n",
      "**********\n",
      "epoch 247\n",
      "Loss: 0.20967384427785873\n",
      "**********\n",
      "epoch 248\n",
      "Loss: 0.20809529721736908\n",
      "**********\n",
      "epoch 249\n",
      "Loss: 0.20653237402439117\n",
      "**********\n",
      "epoch 250\n",
      "Loss: 0.20498494803905487\n",
      "**********\n",
      "epoch 251\n",
      "Loss: 0.2034529447555542\n",
      "**********\n",
      "epoch 252\n",
      "Loss: 0.20193611830472946\n",
      "**********\n",
      "epoch 253\n",
      "Loss: 0.2004343792796135\n",
      "**********\n",
      "epoch 254\n",
      "Loss: 0.19894754886627197\n",
      "**********\n",
      "epoch 255\n",
      "Loss: 0.19747547805309296\n",
      "**********\n",
      "epoch 256\n",
      "Loss: 0.19601795077323914\n",
      "**********\n",
      "epoch 257\n",
      "Loss: 0.1945750042796135\n",
      "**********\n",
      "epoch 258\n",
      "Loss: 0.19314632564783096\n",
      "**********\n",
      "epoch 259\n",
      "Loss: 0.1917317882180214\n",
      "**********\n",
      "epoch 260\n",
      "Loss: 0.19033129513263702\n",
      "**********\n",
      "epoch 261\n",
      "Loss: 0.18894466757774353\n",
      "**********\n",
      "epoch 262\n",
      "Loss: 0.18757180124521255\n",
      "**********\n",
      "epoch 263\n",
      "Loss: 0.18621254712343216\n",
      "**********\n",
      "epoch 264\n",
      "Loss: 0.18486671149730682\n",
      "**********\n",
      "epoch 265\n",
      "Loss: 0.18353421986103058\n",
      "**********\n",
      "epoch 266\n",
      "Loss: 0.18221484869718552\n",
      "**********\n",
      "epoch 267\n",
      "Loss: 0.18090855330228806\n",
      "**********\n",
      "epoch 268\n",
      "Loss: 0.17961516231298447\n",
      "**********\n",
      "epoch 269\n",
      "Loss: 0.1783345565199852\n",
      "**********\n",
      "epoch 270\n",
      "Loss: 0.17706651985645294\n",
      "**********\n",
      "epoch 271\n",
      "Loss: 0.1758110076189041\n",
      "**********\n",
      "epoch 272\n",
      "Loss: 0.17456787824630737\n",
      "**********\n",
      "epoch 273\n",
      "Loss: 0.1733369454741478\n",
      "**********\n",
      "epoch 274\n",
      "Loss: 0.17211811989545822\n",
      "**********\n",
      "epoch 275\n",
      "Loss: 0.17091122269630432\n",
      "**********\n",
      "epoch 276\n",
      "Loss: 0.1697162389755249\n",
      "**********\n",
      "epoch 277\n",
      "Loss: 0.16853290051221848\n",
      "**********\n",
      "epoch 278\n",
      "Loss: 0.16736114770174026\n",
      "**********\n",
      "epoch 279\n",
      "Loss: 0.1662009209394455\n",
      "**********\n",
      "epoch 280\n",
      "Loss: 0.16505199670791626\n",
      "**********\n",
      "epoch 281\n",
      "Loss: 0.16391423344612122\n",
      "**********\n",
      "epoch 282\n",
      "Loss: 0.16278765350580215\n",
      "**********\n",
      "epoch 283\n",
      "Loss: 0.1616719588637352\n",
      "**********\n",
      "epoch 284\n",
      "Loss: 0.16056717932224274\n",
      "**********\n",
      "epoch 285\n",
      "Loss: 0.15947315096855164\n",
      "**********\n",
      "epoch 286\n",
      "Loss: 0.15838968753814697\n",
      "**********\n",
      "epoch 287\n",
      "Loss: 0.15731672942638397\n",
      "**********\n",
      "epoch 288\n",
      "Loss: 0.1562541499733925\n",
      "**********\n",
      "epoch 289\n",
      "Loss: 0.15520185232162476\n",
      "**********\n",
      "epoch 290\n",
      "Loss: 0.15415970981121063\n",
      "**********\n",
      "epoch 291\n",
      "Loss: 0.15312759578227997\n",
      "**********\n",
      "epoch 292\n",
      "Loss: 0.1521054431796074\n",
      "**********\n",
      "epoch 293\n",
      "Loss: 0.15109313279390335\n",
      "**********\n",
      "epoch 294\n",
      "Loss: 0.15009049326181412\n",
      "**********\n",
      "epoch 295\n",
      "Loss: 0.1490975022315979\n",
      "**********\n",
      "epoch 296\n",
      "Loss: 0.14811400324106216\n",
      "**********\n",
      "epoch 297\n",
      "Loss: 0.14713993668556213\n",
      "**********\n",
      "epoch 298\n",
      "Loss: 0.146175105124712\n",
      "**********\n",
      "epoch 299\n",
      "Loss: 0.14521952718496323\n",
      "**********\n",
      "epoch 300\n",
      "Loss: 0.14427301287651062\n",
      "\n",
      "tensor([[-3.2553, -0.1175, -2.6267],\n",
      "        [-2.2993, -2.3949, -0.2126],\n",
      "        [-0.1823, -2.3556, -2.6331],\n",
      "        [-2.8900, -0.0871, -3.5799]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    print('*' * 10)\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    running_loss = 0\n",
    "    for data in training_data:\n",
    "        word, tag = data\n",
    "        word_list = make_sequence(word, word_to_idx)\n",
    "        tag = make_sequence(tag, tag_to_idx)\n",
    "        if torch.cuda.is_available():\n",
    "            word_list = word_list.cuda()\n",
    "            tag = tag.cuda()\n",
    "        # forward\n",
    "        out = model(word_list, word)\n",
    "        loss = criterion(out, tag)\n",
    "        running_loss += loss.data.item()\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {}'.format(running_loss / len(data)))\n",
    "print()\n",
    "input = make_sequence(\"Everybody ate the apple\".split(), word_to_idx)\n",
    "if torch.cuda.is_available():\n",
    "    input = input.cuda()\n",
    "\n",
    "out = model(input, \"Everybody ate the apple\".split())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
