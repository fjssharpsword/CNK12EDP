{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py:200: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    }
   ],
   "source": [
    "from itertools import compress\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "import json\n",
    "from bert_serving.client import BertClient\n",
    "import os.path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image  \n",
    "import ftplib\n",
    "import time\n",
    "import threading\n",
    "\n",
    "#1.加载训练文件\n",
    "with open(\"/data/fjs/data/math/math.json\",'r') as load_f:\n",
    "    allItems = json.load(load_f)#list中条目是dict\n",
    "    \n",
    "#2.探索性数据分析EDA\n",
    "#2.1 习题难度分布\n",
    "diffs={}#统计难度类别\n",
    "for item in allItems:\n",
    "    diff = item['qdiff'] #获取习题难度\n",
    "    diffs[diff]=diffs.get(diff,0)+1 #统计每个元素出现的个数。\n",
    "#{0: 9, 1: 5434, 2: 198597, 3: 482016, 4: 212266, 5: 1678}\n",
    "df = pd.DataFrame(diffs,index=[0])\n",
    "df.plot(kind='bar')\n",
    "plt.show()\n",
    "#2.2 习题概念分布\n",
    "cons={}#统计概念类别\n",
    "for item in allItems:\n",
    "    con =item['qklg']\n",
    "    conp = con.split(',')#逗号分隔\n",
    "    for p in conp:#具体概念点\n",
    "        cons[p]=cons.get(p,0)+1\n",
    "df = pd.DataFrame(list(cons.items()))\n",
    "print (df.shape[0])#知识点数\n",
    "print (np.mean(df.loc[:,1]))#知识点数的均值\n",
    "#绘制箱形图\n",
    "def drawBox(freqs):\n",
    "    #创建箱形图\n",
    "    #第一个参数为待绘制的定量数据\n",
    "    #第二个参数为数据的文字说明\n",
    "    plt.boxplot([freqs], labels=['Concepts'])\n",
    "    plt.title('Frequecy Of Concpets')\n",
    "    plt.show()\n",
    "drawBox(df.loc[:,1])\n",
    "\n",
    "#3.习题的文本、概念基于BERT模型Embedding；习题的图像基于Resnet模型提取视觉特征\n",
    "f = ftplib.FTP(\"10.21.3.24\")  # 实例化FTP对象\n",
    "f.login(\"ftpuser\", \"dm2019\")  # 登录\n",
    "def ftp_download(remotename,localname):\n",
    "    bufsize = 1024  # 设置缓冲器大小\n",
    "    fp = open(localname, 'wb')\n",
    "    f.retrbinary('RETR %s' % remotename, fp.write, bufsize)\n",
    "    fp.close()\n",
    "#预训练resnet50模型\n",
    "transform1 = transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()])\n",
    "resnet50_feature_extractor = models.resnet50(pretrained = True)\n",
    "resnet50_feature_extractor.fc = nn.Linear(2048, 2048)\n",
    "torch.nn.init.eye(resnet50_feature_extractor.fc.weight)\n",
    "for param in resnet50_feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "#定义18个线程提取特征，每个线程提取5万条\n",
    "def threadFunction(n,items):\n",
    "    print (\"Start Threading %d.\"%n)\n",
    "    start = time.time()\n",
    "    VItems = []#保存列表\n",
    "    for item in items:\n",
    "        VItem={}#dict\n",
    "        VItem['quid']=item['quid']\n",
    "        VItem['qdiff']=item['qdiff']\n",
    "        #3.1 文本、概念embedding\n",
    "        texts=item['qtext']#list\n",
    "        strtext=''\n",
    "        for text in texts:\n",
    "            strtext=strtext+text\n",
    "        concepts=item['qklg']\n",
    "        bc = BertClient()#连接BERT Server\n",
    "        VItem['qtext']=bc.encode([strtext])[0]#list,768长度\n",
    "        VItem['qklg']=bc.encode([concepts])[0]#list，768长度\n",
    "        #3.2 图片视觉特征提取\n",
    "        images=[]\n",
    "        for imgftppath in item['qimage']:\n",
    "            imgftppath = imgftppath[17:]#截掉ftp://10.21.3.24/\n",
    "            imglocalpath = \"/data/fjs/data/imgs/\"+imgftppath.split(\"/\")[-1]\n",
    "            ftp_download(imgftppath,imglocalpath)#从ftp下载到本地\n",
    "            image = Image.open(imglocalpath)#打开本地图片\n",
    "            images.append(image)\n",
    "        #多图拼接成一张长图\n",
    "        if len(images)==0:\n",
    "            VItem['qimage']=np.zeros(2048)\n",
    "        else:\n",
    "            width,height=images[0].size\n",
    "            for image in images:\n",
    "                w,h = image.size\n",
    "                if w>width:width=w\n",
    "                if h>height: height=h\n",
    "            longImg =  Image.new(images[0].mode,(width,height*len(images)))\n",
    "            for i,im in enumerate(images):\n",
    "                longImg.paste(im,box=(0,i*height))\n",
    "            #长图向量化    \n",
    "            imgarr = np.array(longImg)\n",
    "            if imgarr.shape[2] == 4: #四通道转为三通道\n",
    "                img1 = longImg.convert(\"RGB\")\n",
    "            #img = Image.fromarray(img.astype('uint8')).convert('RGB')\n",
    "            img2 = transform1(img1)\n",
    "            x = Variable(torch.unsqueeze(img2, dim=0).float(), requires_grad=False)\n",
    "            y = resnet50_feature_extractor(x)\n",
    "            y = y.data.numpy()\n",
    "            VItem['qimage']=y #2048长度\n",
    "        VItems.append(VItem)\n",
    "    with open(\"/data/fjs/data/math/mathV\"+str(n)+\".json\",\"w\") as dump_f:\n",
    "        json.dump(VItems,dump_f)#保存到json文件\n",
    "    print ('file %d saved.'%n)\n",
    "    end = time.time()\n",
    "    print (\"Complete time: %f s\" % (end - start))\n",
    "\n",
    "#将全部90万条记录分成18份提取特征\n",
    "for i in range(0,len(allItems),50000):\n",
    "    mThread = threading.Thread(target=threadFunction, args=(i,allItems[i:i+50000]))\n",
    "    mThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import json \n",
    "import ftplib\n",
    "from pathlib import Path\n",
    "#1.加载训练文件\n",
    "with open(\"/data/fjs/data/math/math.json\",'r') as load_f:\n",
    "    allItems = json.load(load_f)#list中条目是dict\n",
    "    \n",
    "#count =0\n",
    "f = ftplib.FTP(\"10.21.3.24\")  # 实例化FTP对象\n",
    "f.login(\"ftpuser\", \"dm2019\")  # 登录\n",
    "def ftp_download(remotename,localname):\n",
    "    bufsize = 1024  # 设置缓冲器大小\n",
    "    fp = open(localname, 'wb')\n",
    "    f.retrbinary('RETR %s' % remotename, fp.write, bufsize)\n",
    "    fp.close()\n",
    "#将ftp全部手动在操作系统上下载到本地，然后再调整异常的，减少磁盘IO和网络传输时间\n",
    "imgs=[]\n",
    "for item in allItems:\n",
    "    for img in item['qimage']:\n",
    "        #if img.find('ftp://10.21.3.24/')==-1:print (img)\n",
    "        #if img.find('upimages')==-1:print (img)\n",
    "        ipath = img[img.find('upimages'):]#截取ftp://10.21.3.24/tiku/images/后的目录\n",
    "        ipath = \"/data/fjs/data/mathimgs/\"+ ipath\n",
    "        #if img.find('\\\\')>=0:img = img[:-1] #去除最后一个字符\n",
    "        if ipath.find('\\\\')>=0:ipath = ipath[:-1] #去除最后一个字符\n",
    "        if not os.path.exists(ipath):#本地不存在，去下载。\n",
    "            img = img[17:]#截掉ftp://10.21.3.24/\n",
    "            imgs.append(img)\n",
    "for img in imgs:\n",
    "    ipath = img[img.find('upimages'):]\n",
    "    ipath = \"/data/fjs/data/mathimgs/\"+ ipath\n",
    "    ftp_download(img,ipath)#从ftp下载到本地 \n",
    "    print (ipath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900000, 5)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "#将list中dict的json文件转化为DataFrame并保存为CSV，可以利用python的矩阵计算优势，而不是采用循环\n",
    "with open(\"/data/fjs/data/math/math.json\",'r') as load_f:\n",
    "    allItems = json.load(load_f)#list中条目是dict\n",
    "csvlist=[]\n",
    "for item in allItems:\n",
    "    strtext=''\n",
    "    for text in item['qtext']: strtext=strtext+text\n",
    "    strimage=''\n",
    "    for image in item['qimage']:strimage=strimage+\",\"+image\n",
    "    tmplist=[item['quid'],item['qdiff'],item['qklg'], strtext, strimage]\n",
    "    csvlist.append(tmplist)\n",
    "df = pd.DataFrame(csvlist, columns=['quid', 'qdiff', 'qklg','qtext','qimage']) \n",
    "print (df.shape)\n",
    "df.to_csv('/data/fjs/data/math/math.csv',index=False,sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234921, 5)\n"
     ]
    }
   ],
   "source": [
    "#去除空置，在多模态学习上更有意义。\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/data/fjs/data/math/math.csv\",sep='|') #print (data.head())\n",
    "data=data.dropna(axis=0,how='any')#删除包含空置的行，主要是没有图片。\n",
    "#data = data.fillna('none')\n",
    "print (data.shape)\n",
    "#print (data.head())\n",
    "data.to_csv('/data/fjs/data/math/mathmin.csv',index=False,sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py:200: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.eye is now deprecated in favor of nn.init.eye_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234921 rows and 5 cols\n",
      "5 rows and 772 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fjs/.local/lib/python3.5/site-packages/bert_serving/client/__init__.py:277: UserWarning: server does not put a restriction on \"max_seq_len\", it will determine \"max_seq_len\" dynamically according to the sequences in the batch. you can restrict the sequence length on the client side for better efficiency\n",
      "  warnings.warn('server does not put a restriction on \"max_seq_len\", '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 rows and 1539 cols\n",
      "5 rows and 3586 cols\n",
      "Complete time: 6.398251 s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from bert_serving.client import BertClient\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models,transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#1.预训练resnet50模型构建\n",
    "transform1 = transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()])\n",
    "resnet50_feature_extractor = models.resnet50(pretrained = True)\n",
    "resnet50_feature_extractor.fc = nn.Linear(2048, 2048)\n",
    "torch.nn.init.eye(resnet50_feature_extractor.fc.weight)\n",
    "for param in resnet50_feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#2.连接BertService\n",
    "bc = BertClient()#连接BERT Server\n",
    "\n",
    "#3.多图拼接长图并向量化函数定义\n",
    "def fn_ImageToVec(x):\n",
    "    images=[]\n",
    "    for img in x.split(\",\")[1:]:#ftp图片用逗号隔开\n",
    "        try:\n",
    "            if img.find('\\\\'): img=img.replace('\\\\','')\n",
    "            ipath = img[img.find('upimages'):]#截取ftp://10.21.3.24/tiku/images/后的目录\n",
    "            ipath = \"/data/fjs/data/mathimgs/\"+ ipath\n",
    "            image = Image.open(ipath)#打开本地图片\n",
    "            images.append(image)\n",
    "        except OSError:pass\n",
    "        continue\n",
    "    if len(images)==0:return np.zeros((1,2048))\n",
    "    else:\n",
    "        #多图拼接成一张长图\n",
    "        width,height=images[0].size\n",
    "        for image in images:\n",
    "            w,h = image.size\n",
    "            if w>width:width=w\n",
    "            if h>height: height=h\n",
    "        longImg =  Image.new(images[0].mode,(width,height*len(images)))\n",
    "        for i,im in enumerate(images):\n",
    "            longImg.paste(im,box=(0,i*height))\n",
    "        #长图向量化    \n",
    "        imgarr = np.array(longImg)\n",
    "        if imgarr.shape[2] == 4: #四通道转为三通道\n",
    "            longImg = longImg.convert(\"RGB\")\n",
    "        longImg = transform1(longImg)\n",
    "        x = Variable(torch.unsqueeze(longImg, dim=0).float(), requires_grad=False)\n",
    "        y = resnet50_feature_extractor(x)\n",
    "        return y.data.numpy() #2048长度\n",
    "\n",
    "start = time.time()    \n",
    "#4.加载训练文件\n",
    "data = pd.read_csv(\"/data/fjs/data/math/mathmin.csv\",sep='|') \n",
    "print (\"%d rows and %d cols\"%(data.shape[0],data.shape[1]))\n",
    "#data = data.head()\n",
    "#5.1概念向量化\n",
    "vecklg = bc.encode(data['qklg'].tolist())\n",
    "data = pd.concat([data,pd.DataFrame(vecklg)],axis=1)#768维\n",
    "data=data.drop(['qklg'], axis=1)\n",
    "print (\"%d rows and %d cols\"%(data.shape[0],data.shape[1]))\n",
    "#5.2文本向量化\n",
    "vectxt = data['qtext'].apply(lambda x: bc.encode([x])[0])\n",
    "data = pd.concat([data,pd.DataFrame(vectxt.tolist())],axis=1)#768维\n",
    "data=data.drop(['qtext'], axis=1)\n",
    "print (\"%d rows and %d cols\"%(data.shape[0],data.shape[1]))\n",
    "#5.3 图像向量化\n",
    "npvimg=data['qimage'].apply(lambda x: fn_ImageToVec(x)[0])\n",
    "data = pd.concat([data,pd.DataFrame(npvimg.tolist())],axis=1)#2048维\n",
    "data=data.drop(['qimage'], axis=1)\n",
    "print (\"%d rows and %d cols\"%(data.shape[0],data.shape[1]))\n",
    "#6.保存向量化后的文件\n",
    "data.to_csv(\"/data/fjs/data/math/mathV.csv\",index=False,sep='|')\n",
    "end = time.time()\n",
    "print (\"Complete time: %f s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py:200: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.eye is now deprecated in favor of nn.init.eye_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234921 rows and 5 cols\n",
      "(100, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fjs/.local/lib/python3.5/site-packages/bert_serving/client/__init__.py:277: UserWarning: server does not put a restriction on \"max_seq_len\", it will determine \"max_seq_len\" dynamically according to the sequences in the batch. you can restrict the sequence length on the client side for better efficiency\n",
      "  warnings.warn('server does not put a restriction on \"max_seq_len\", '\n",
      "Exception ignored in: <bound method Context.__del__ of <zmq.sugar.context.Context object at 0x7fcbd290fba8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fjs/.local/lib/python3.5/site-packages/zmq/sugar/context.py\", line 46, in __del__\n",
      "    self.term()\n",
      "  File \"zmq/backend/cython/context.pyx\", line 136, in zmq.backend.cython.context.Context.term\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 rows and 3586 cols\n",
      "file 0 Complete time: 120.758166 s\n",
      "(100, 5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a51d843e44c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qtext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#5.3 图像向量化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mnpvimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qimage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfn_ImageToVec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpvimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#2048维\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qimage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a51d843e44c0>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qtext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#5.3 图像向量化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mnpvimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qimage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfn_ImageToVec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpvimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#2048维\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qimage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a51d843e44c0>\u001b[0m in \u001b[0;36mfn_ImageToVec\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mlongImg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongImg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongImg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#2048长度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision-0.2.1-py3.5.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision-0.2.1-py3.5.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from bert_serving.client import BertClient\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models,transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#1.预训练resnet50模型构建\n",
    "transform1 = transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()])\n",
    "resnet50_feature_extractor = models.resnet50(pretrained = True)\n",
    "resnet50_feature_extractor.fc = nn.Linear(2048, 2048)\n",
    "torch.nn.init.eye(resnet50_feature_extractor.fc.weight)\n",
    "for param in resnet50_feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#2.连接BertService\n",
    "bc = BertClient()#连接BERT Server\n",
    "\n",
    "#3.多图拼接长图并向量化函数定义\n",
    "def fn_ImageToVec(x):\n",
    "    images=[]\n",
    "    for img in x.split(\",\")[1:]:#ftp图片用逗号隔开\n",
    "        if img.find('\\\\'): img=img.replace('\\\\','')\n",
    "        ipath = img[img.find('upimages'):]#截取ftp://10.21.3.24/tiku/images/后的目录\n",
    "        ipath = \"/data/fjs/data/mathimgs/\"+ ipath\n",
    "        image = Image.open(ipath)#打开本地图片\n",
    "        images.append(image)\n",
    "\n",
    "    if len(images)==0:return np.zeros((1,2048))\n",
    "    else:\n",
    "        #多图拼接成一张长图\n",
    "        width,height=images[0].size\n",
    "        for image in images:\n",
    "            w,h = image.size\n",
    "            if w>width:width=w\n",
    "            if h>height: height=h\n",
    "        longImg =  Image.new(images[0].mode,(width,height*len(images)))\n",
    "        for i,im in enumerate(images):\n",
    "            longImg.paste(im,box=(0,i*height))\n",
    "        #长图向量化    \n",
    "        imgarr = np.array(longImg)\n",
    "        if imgarr.shape[2] == 4: #四通道转为三通道\n",
    "            longImg = longImg.convert(\"RGB\")\n",
    "        longImg = transform1(longImg)\n",
    "        x = Variable(torch.unsqueeze(longImg, dim=0).float(), requires_grad=False)\n",
    "        y = resnet50_feature_extractor(x)\n",
    "        return y.data.numpy() #2048长度\n",
    "\n",
    "#4.加载训练文件\n",
    "dataIn = pd.read_csv(\"/data/fjs/data/math/mathmin.csv\",sep='|') \n",
    "print (\"%d rows and %d cols\"%(dataIn.shape[0],dataIn.shape[1]))\n",
    "\n",
    "for i in range(0,dataIn.shape[0],100):\n",
    "    try:\n",
    "        start = time.time() \n",
    "        data=dataIn[i:i+100]\n",
    "        print (data.shape)\n",
    "        #5.1概念向量化\n",
    "        vecklg = bc.encode(data['qklg'].tolist())\n",
    "        data = pd.concat([data,pd.DataFrame(vecklg)],axis=1)#768维\n",
    "        data=data.drop(['qklg'], axis=1)\n",
    "        #5.2文本向量化\n",
    "        vectxt = data['qtext'].apply(lambda x: bc.encode([str(x)])[0])\n",
    "        data = pd.concat([data,pd.DataFrame(vectxt.tolist())],axis=1)#768维\n",
    "        data=data.drop(['qtext'], axis=1)\n",
    "        #5.3 图像向量化\n",
    "        npvimg=data['qimage'].apply(lambda x: fn_ImageToVec(str(x))[0])\n",
    "        data = pd.concat([data,pd.DataFrame(npvimg.tolist())],axis=1)#2048维\n",
    "        data=data.drop(['qimage'], axis=1)\n",
    "        #6.保存向量化后的文件\n",
    "        print (\"%d rows and %d cols\"%(data.shape[0],data.shape[1]))\n",
    "        data.to_csv(\"/data/fjs/data/math/max/mathv\"+str(i)+\".csv\",index=False,sep='|')\n",
    "        end = time.time()\n",
    "        print (\"file %d Complete time: %f s\" % (i,(end - start)))\n",
    "    except Exception:pass\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
